{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "import urllib.request\n",
    "import sys\n",
    "from six.moves import urllib\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download_and_extract(dest_directory, url):\n",
    "  if not os.path.exists(dest_directory):\n",
    "    os.makedirs(dest_directory)\n",
    "  file_name = 'cifar-10-binary.tar.gz'\n",
    "  file_path = os.path.join(dest_directory, file_name)\n",
    "  # if have not downloaded yet\n",
    "  if not os.path.exists(file_path):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r%.1f%%' % \n",
    "            (float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()  # flush the buffer\n",
    "\n",
    "    print('>> Downloading %s ...' % file_name)\n",
    "    file_path, _ = urllib.request.urlretrieve(url, file_path, _progress)\n",
    "    file_size = os.stat(file_path).st_size\n",
    "    print('\\r>> Total %d bytes' % file_size)\n",
    "  extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n",
    "  if not os.path.exists(extracted_dir_path):\n",
    "    # Open for reading with gzip compression, then extract all\n",
    "    tarfile.open(file_path, 'r:gz').extractall(dest_directory)\n",
    "  print('>> Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Done\n"
     ]
    }
   ],
   "source": [
    "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "DEST_DIRECTORY = 'dataset/cifar10'\n",
    "DATA_DIRECTORY = DEST_DIRECTORY + '/cifar-10-batches-bin'\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_DEPTH = 3\n",
    "IMAGE_SIZE_CROPPED = 24\n",
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = 10 \n",
    "LABEL_BYTES = 1\n",
    "IMAGE_BYTES = 32 * 32 * 3\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "# download it\n",
    "maybe_download_and_extract(DEST_DIRECTORY, DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the folder store the dataset\n",
    "DATA_DIRECTORY = DEST_DIRECTORY + '/cifar-10-batches-bin'\n",
    "# (1) a list of training/testing filenames\n",
    "training_files = [os.path.join(DATA_DIRECTORY, 'data_batch_%d.bin' % i) for i in range(1,6)]\n",
    "testing_files = [os.path.join(DATA_DIRECTORY, 'test_batch.bin')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar10_record_distort_parser(record):\n",
    "    ''' Parse the record into label, cropped and distorted image\n",
    "    -----\n",
    "    Args:\n",
    "        record: \n",
    "            a record containing label and image.\n",
    "    Returns:\n",
    "        label: \n",
    "            the label in the record.\n",
    "        image: \n",
    "            the cropped and distorted image in the record.\n",
    "    '''\n",
    "    # TODO1\n",
    "    label, distorted_image = cifar10_record_crop_parser(record)\n",
    "    distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "    distorted_image = tf.image.random_brightness(distorted_image, max_delta=63)\n",
    "    distorted_image = tf.image.random_contrast(distorted_image, lower=0.2, upper=1.8)\n",
    "    distorted_image = tf.image.per_image_standardization(distorted_image)\n",
    "    distorted_image.set_shape([IMAGE_SIZE_CROPPED, IMAGE_SIZE_CROPPED, 3])\n",
    "    return label,  distorted_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar10_record_crop_parser(record):\n",
    "    ''' Parse the record into label, cropped image\n",
    "    -----\n",
    "    Args:\n",
    "        record: \n",
    "            a record containing label and image.\n",
    "    Returns:\n",
    "        label: \n",
    "            the label in the record.\n",
    "        image: \n",
    "            the cropped image in the record.\n",
    "    '''\n",
    "    # TODO2\n",
    "    record_bytes = tf.decode_raw(record, tf.uint8)\n",
    "    label = tf.cast(tf.slice(record_bytes, [0], [LABEL_BYTES]), tf.int32)\n",
    "    depth_major = tf.reshape(tf.slice(record_bytes,[LABEL_BYTES], [IMAGE_BYTES]),[IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    float_image = tf.transpose(depth_major, [1, 2, 0])\n",
    "\n",
    "    float_image = tf.random_crop(float_image, [IMAGE_SIZE_CROPPED, IMAGE_SIZE_CROPPED, 3])\n",
    "\n",
    "    return label, float_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar10_iterator(filenames, batch_size, cifar10_record_parser):\n",
    "    ''' Create a dataset and return a tf.contrib.data.Iterator \n",
    "    which provides a way to extract elements from this dataset.\n",
    "    -----\n",
    "    Args:\n",
    "        filenames: \n",
    "            a tensor of filenames.\n",
    "        batch_size: \n",
    "            batch size.\n",
    "    Returns:\n",
    "        iterator: \n",
    "            an Iterator providing a way to extract elements from the created dataset.\n",
    "        output_types: \n",
    "            the output types of the created dataset.\n",
    "        output_shapes: \n",
    "            the output shapes of the created dataset.\n",
    "    '''\n",
    "\n",
    "    # TODO3\n",
    "    # tips: use dataset.map with cifar10_record_parser(record)\n",
    "    #       output_types = dataset.output_types\n",
    "    #       output_shapes = dataset.output_shapes\n",
    "    #reading files\n",
    "    record_bytes = LABEL_BYTES + IMAGE_BYTES\n",
    "    dataset = tf.data.FixedLengthRecordDataset(filenames, record_bytes)\n",
    "    dataset = dataset.map(lambda record : cifar10_record_parser(record))\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "    return iterator, output_types, output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hps_cifar = tf.contrib.training.HParams(\n",
    "  image_size = IMAGE_SIZE_CROPPED,\n",
    "  batch_size = BATCH_SIZE,\n",
    "  num_classes = NUM_CLASSES,\n",
    "  num_training_example = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN,\n",
    "  num_epoch_per_decay = 350.0,\n",
    "  init_lr = 0.1,\n",
    "  moving_average_decay = 0.9999,\n",
    "  ckpt_dir = './check/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(object):\n",
    "  def __init__(self, model_hps):\n",
    "    self.image_size = model_hps.image_size\n",
    "    self.batch_size = model_hps.batch_size\n",
    "    self.num_classes = model_hps.num_classes\n",
    "    self.num_training_example = model_hps.num_training_example\n",
    "    self.num_epoch_per_decay = model_hps.num_epoch_per_decay\n",
    "    self.init_lr = model_hps.init_lr  # initial learn rate\n",
    "    self.moving_average_decay = model_hps.moving_average_decay\n",
    "    self.ckpt_dir = model_hps.ckpt_dir\n",
    "    \n",
    "    self.build_model()\n",
    "    \n",
    "  def build_model(self):\n",
    "    # op for training\n",
    "    self.global_step = tf.train.get_or_create_global_step()\n",
    "        \n",
    "    with tf.variable_scope('model'):\n",
    "      self.images = tf.placeholder(tf.float32,[self.batch_size, self.image_size, self.image_size, 3]) \n",
    "      self.labels = tf.placeholder(tf.int32)\n",
    "        \n",
    "      self.logits = self.inference(self.images)\n",
    "      self.top_k_op = tf.nn.in_top_k(self.logits, self.labels, 1) \n",
    "      self.total_loss = self.loss(self.logits, self.labels)\n",
    "      self.train_op = self.train(self.total_loss, self.global_step)\n",
    "    \n",
    "  def _variable_on_cpu(self, name, shape, initializer):\n",
    "    with tf.device('/cpu:0'):\n",
    "        var = tf.get_variable(name, shape, initializer=initializer, dtype=tf.float32)\n",
    "    \n",
    "    return var\n",
    "\n",
    "  def _variable_with_weight_decay(self, name, shape, stddev, wd=0.0):\n",
    "    \"\"\" Helper to create an initialized Variable with weight decay.\n",
    "        Note that the Variable is initialized with a truncated normal \n",
    "        distribution. A weight decay is added only if one is specified.\n",
    "        -----\n",
    "        Args:\n",
    "            name: \n",
    "                name of the variable\n",
    "            shape: \n",
    "                a list of ints\n",
    "            stddev: \n",
    "                standard deviation of a truncated Gaussian\n",
    "            wd: \n",
    "                add L2Loss weight decay multiplied by this float. If None, weight\n",
    "                decay is not added for this Variable.\n",
    "        Returns:\n",
    "            Variable Tensor\n",
    "    \"\"\"\n",
    "    initializer = tf.truncated_normal_initializer(\n",
    "        stddev=stddev, dtype=tf.float32)\n",
    "    var = self._variable_on_cpu(name, shape, initializer)\n",
    "    # deal with weight decay\n",
    "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "    tf.add_to_collection('losses', weight_decay)\n",
    "    return var\n",
    "\n",
    "  def _conv_block(self, inp, scope, kernel_width, kernel_height, inp_channel, out_channel, strides = [1, 1, 1, 1], padding='SAME'):\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "      kernel = self._variable_with_weight_decay('weights', [kernel_width, kernel_width, inp_channel, out_channel], 5e-2)\n",
    "      biases = self._variable_on_cpu('bias', [out_channel], tf.constant_initializer(0.0))\n",
    "\n",
    "      conv = tf.nn.conv2d(inp, kernel, strides=strides, padding=padding)\n",
    "      pre_activation = tf.nn.bias_add(conv, biases)\n",
    "      return tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "  def _fully_connected_layer(self, inp, scope, in_dim, out_dim, relu = True):\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "      weights = self._variable_with_weight_decay('weights', [in_dim, out_dim], 0.04, 0.004)\n",
    "      biases = self._variable_on_cpu('biases', [out_dim], tf.constant_initializer(0.1))\n",
    "      if relu:\n",
    "        return tf.nn.relu(tf.matmul(inp, weights) + biases, name=scope.name)\n",
    "      else:\n",
    "        return tf.matmul(inp, weights) + biases\n",
    "    \n",
    "  def inference(self, images):\n",
    "    \"\"\" build the model\n",
    "        -----\n",
    "        Args:\n",
    "            images with shape [batch_size,24,24,3]\n",
    "        Return:\n",
    "            logits with shape [batch_size,10]\n",
    "    \"\"\"\n",
    "    conv_1 = self._conv_block(images, 'conv_1', 5, 5, 3, 64)\n",
    "    # pool_1\n",
    "    pool_1 = tf.nn.max_pool(conv_1,ksize=[1, 3, 3, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool_1')\n",
    "    # norm_1 (local_response_normalization)\n",
    "    norm_1 = tf.nn.lrn(pool_1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm_1')\n",
    "    \n",
    "    # conv2\n",
    "    conv_2 = self._conv_block(norm_1, 'conv_2', 5, 5, 64, 64)\n",
    "    # norm2\n",
    "    norm_2 = tf.nn.lrn(conv_2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm_2')\n",
    "    # pool2\n",
    "    pool_2 = tf.nn.max_pool(norm_2,ksize=[1, 3, 3, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool_2')\n",
    "    \n",
    "    # Flatten feature maps before fully connected layers\n",
    "    flat_features = tf.reshape(pool_2, [self.batch_size, -1])\n",
    "    dim = flat_features.get_shape()[1].value\n",
    "    # FC_1 (fully-connected layer)\n",
    "    fc_1 = self._fully_connected_layer(flat_features, 'fc1', dim, 384)\n",
    "\n",
    "    # FC_2\n",
    "    fc_2 = self._fully_connected_layer(fc_1, 'fc2', 384, 192)\n",
    "\n",
    "    logits = self._fully_connected_layer(fc_2, 'softmax_linear', 192, self.num_classes, relu = False)\n",
    "    return logits\n",
    "\n",
    "  def loss(self, logits, labels):\n",
    "    '''calculate the loss'''\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels, logits=logits, name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    # The total loss is defined as the cross entropy loss plus all of the weight\n",
    "    # decay terms (L2 loss).\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "  def train(self, total_loss, global_step):\n",
    "    '''Return training operation of one step'''\n",
    "    num_batches_per_epoch = self.num_training_example / self.batch_size\n",
    "    decay_steps = int(num_batches_per_epoch * self.num_epoch_per_decay)\n",
    "    # Decay the learning rate exponentially based on the number of steps.\n",
    "    lr = tf.train.exponential_decay(\n",
    "        self.init_lr, global_step, decay_steps, decay_rate=0.1, staircase=True)\n",
    "    opt = tf.train.GradientDescentOptimizer(lr)\n",
    "    grads = opt.compute_gradients(total_loss)\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "    # Track the moving averages of all trainable variables.\n",
    "    # This step just records the moving average weights but not uses them\n",
    "    ema = tf.train.ExponentialMovingAverage(self.moving_average_decay,\n",
    "                                            global_step)\n",
    "    self.ema = ema\n",
    "    variables_averages_op = ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "      train_op = tf.no_op(name='train')\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#define training/testing data files\n",
    "training_files = [os.path.join(DATA_DIRECTORY, 'data_batch_%d.bin' % i) for i in range(1, 6)]\n",
    "testing_files = [os.path.join(DATA_DIRECTORY, 'test_batch.bin')]\n",
    "filenames_train = tf.constant(training_files)\n",
    "filenames_test = tf.constant(testing_files)\n",
    "# Training data iterator\n",
    "iterator_train, types, shapes = cifar10_iterator(filenames_train, BATCH_SIZE, cifar10_record_distort_parser)\n",
    "# Testing data iterator\n",
    "iterator_test, _, _ = cifar10_iterator(filenames_test, BATCH_SIZE, cifar10_record_crop_parser)\n",
    "\n",
    "# use to handle training and testing\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(handle, types, shapes)\n",
    "labels_images_pairs = iterator.get_next()\n",
    "# CNN model\n",
    "model = CNN_Model(model_hps_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of epoch 1: 1694.857178\n",
      "loss of epoch 2: 1481.055420\n",
      "loss of epoch 3: 1324.735107\n",
      "loss of epoch 4: 1210.206787\n",
      "loss of epoch 5: 1126.584839\n",
      "loss of epoch 6: 1065.233887\n",
      "loss of epoch 7: 1020.121521\n",
      "loss of epoch 8: 987.535156\n",
      "loss of epoch 9: 963.388123\n",
      "loss of epoch 10: 945.895630\n",
      "CPU times: user 1h 40min 18s, sys: 8min 49s, total: 1h 49min 8s\n",
      "Wall time: 35min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NUM_EPOCH = 10\n",
    "NUM_BATCH_PER_EPOCH = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN // BATCH_SIZE\n",
    "ckpt_dir = './check/'\n",
    "saver = tf.train.Saver()\n",
    "next_train = iterator_train.get_next()\n",
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "    if (ckpt and ckpt.model_checkpoint_path):\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        # assume the name of checkpoint is like '.../model.ckpt-1000'\n",
    "        gs = int(ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1])\n",
    "        sess.run(tf.assign(global_step, gs))\n",
    "    else:\n",
    "        # no checkpoint found\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    for i in range(NUM_EPOCH):\n",
    "        model.loss_each_epoch = []\n",
    "#         sess.run(iterator_train.initializer)\n",
    "        for _ in range(NUM_BATCH_PER_EPOCH):\n",
    "            labels,images = sess.run(next_train)\n",
    "            l, _ = sess.run([model.total_loss, model.train_op],feed_dict={ model.images : images , model.labels : labels.ravel()})\n",
    "            model.loss_each_epoch.append(l)\n",
    "        loss_this_epoch = np.sum(model.loss_each_epoch)\n",
    "        gs = model.global_step.eval()\n",
    "        print('loss of epoch %d: %f' % (gs/NUM_BATCH_PER_EPOCH, loss_this_epoch))\n",
    "        model.loss_each_epoch.append(loss_this_epoch)\n",
    "        saver.save(sess, ckpt_dir + 'model.ckpt', global_step=gs)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./check/model.ckpt-3900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./check/model.ckpt-3900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurarcy: 1132/9984 = 0.113381\n",
      "CPU times: user 37.6 s, sys: 2.87 s, total: 40.4 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "variables_to_restore = model.ema.variables_to_restore()\n",
    "saver = tf.train.Saver(variables_to_restore)\n",
    "next_test = iterator_test.get_next()\n",
    "with tf.Session() as sess:\n",
    "  # Restore variables from disk.\n",
    "  ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "  if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    num_iter = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL // BATCH_SIZE\n",
    "    total_sample_count = num_iter * BATCH_SIZE\n",
    "    true_count = 0\n",
    "    for _ in range(num_iter):\n",
    "      labels , images = sess.run(next_test)\n",
    "      predictions = sess.run(model.top_k_op,feed_dict={model.images : images , model.labels : labels.ravel()})\n",
    "      true_count += np.sum(predictions)\n",
    "    print('Accurarcy: %d/%d = %f' % (true_count, total_sample_count,\n",
    "                                     true_count / total_sample_count))\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
