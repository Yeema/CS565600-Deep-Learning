{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><b>Set GPU environment</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><b>Data Preprocessing ( Cornell Movie-Dialogs )</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
      "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\n"
     ]
    }
   ],
   "source": [
    "# load lines dictionary \n",
    "lines = open('hw13_data/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "# load conversations\n",
    "convs = open('hw13_data/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "print('\\n'.join(lines[:3]))\n",
    "print()\n",
    "print('\\n'.join(convs[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_convs = {}\n",
    "for line in lines:\n",
    "    map_convs[line.split(\"+++$+++\")[0].strip()] = line.split(\"+++$+++\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n"
     ]
    }
   ],
   "source": [
    "for conv in eval(convs[0].split(\"+++$+++\")[-1]):\n",
    "    print(map_convs[conv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the sentences into questions and answers\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in convs:\n",
    "    if len(conv.split(\"+++$+++\")) == 4:\n",
    "        conv_list = eval(conv.split(\"+++$+++\")[-1])\n",
    "        for id,sent in enumerate(conv_list[:-1]):\n",
    "            questions.append(map_convs[sent])\n",
    "            answers.append(map_convs[conv_list[id+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/nlplab/yeema/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = ' '.join(nltk.sent_tokenize(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "    \n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for question in clean_questions:\n",
    "    lengths.append(len(question.split()))\n",
    "for answer in clean_answers:\n",
    "    lengths.append(len(answer.split()))\n",
    "\n",
    "lengths = pd.DataFrame(lengths, columns=['lengths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>443232.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.872094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.215895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>555.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             lengths\n",
       "count  443232.000000\n",
       "mean       10.872094\n",
       "std        12.215895\n",
       "min         0.000000\n",
       "25%         4.000000\n",
       "50%         7.000000\n",
       "75%        14.000000\n",
       "max       555.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "19.0\n",
      "24.0\n",
      "32.0\n",
      "58.0\n"
     ]
    }
   ],
   "source": [
    "print(np.percentile(lengths, 80))\n",
    "print(np.percentile(lengths, 85))\n",
    "print(np.percentile(lengths, 90))\n",
    "print(np.percentile(lengths, 95))\n",
    "print(np.percentile(lengths, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove questions and answers that are shorter than 2 words and longer than 20 words.\n",
    "min_line_length = 1\n",
    "max_line_length = 14\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "for question,answer in zip(clean_questions,clean_answers):\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length and len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "            short_questions.append(question)\n",
    "            short_answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_short_questions = ' '.join([s for s in short_questions])\n",
    "str_short_answers = ' '.join([s for s in short_answers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(str_short_questions.split()+str_short_answers.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we want to use a different vocabulary sizes for the source and target text, \n",
    "# we can set different threshold values.\n",
    "# Nonetheless, we will create dictionaries to provide a unique integer for each word.\n",
    "threshold = 8\n",
    "questions_vocab_to_int = {}\n",
    "answers_vocab_to_int = {}\n",
    "\n",
    "for id, (word, count) in enumerate(vocab.items()):\n",
    "    if count >= threshold:\n",
    "        questions_vocab_to_int[word] = len(questions_vocab_to_int)\n",
    "        answers_vocab_to_int[word] = len(answers_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the unique tokens to the vocabulary dictionaries.\n",
    "codes = ['<PAD>','<END>','<UNK>','<BEG>']\n",
    "\n",
    "for code in codes:\n",
    "    questions_vocab_to_int[code] = len(questions_vocab_to_int)\n",
    "    \n",
    "for code in codes:\n",
    "    answers_vocab_to_int[code] = len(answers_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse dictionary for vocab_to_int.\n",
    "questions_int_to_vocab = {v_i: v for v, v_i in questions_vocab_to_int.items()}\n",
    "answers_int_to_vocab = {v_i: v for v, v_i in answers_vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_questions = ['<BEG> '+s+' <END>' for s in short_questions]\n",
    "short_answers = ['<BEG> '+s+' <END>' for s in short_answers]\n",
    "# short_answers = [s+' <END>' for s in short_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text to integers. \n",
    "# Replace any words that are not in the respective vocabulary with <UNK> \n",
    "questions_int = []\n",
    "for question in short_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questions_vocab_to_int:\n",
    "            ints.append(questions_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(questions_vocab_to_int[word])\n",
    "    questions_int.append(ints)\n",
    "    \n",
    "answers_int = []\n",
    "for answer in short_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answers_vocab_to_int:\n",
    "            ints.append(answers_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(answers_vocab_to_int[word])\n",
    "    answers_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BEG> well i thought we would start with <UNK> if that is okay with you <END>\n",
      "<BEG> well i thought we would start with pronunciation if that is okay with you <END>\n",
      "<BEG> not the hacking and <UNK> and spitting part please <END>\n",
      "<BEG> not the hacking and gagging and spitting part  please <END>\n",
      "\n",
      "<BEG> not the hacking and <UNK> and spitting part please <END>\n",
      "<BEG> not the hacking and gagging and spitting part  please <END>\n",
      "<BEG> okay then how about we try out some french <UNK> saturday night <END>\n",
      "<BEG> okay then how about we try out some french cuisine  saturday  night <END>\n",
      "\n",
      "<BEG> you are asking me out that is so cute that is your name again <END>\n",
      "<BEG> you are asking me out  that is so cute that is your name again <END>\n",
      "<BEG> forget it <END>\n",
      "<BEG> forget it <END>\n",
      "\n",
      "<BEG> no no it is my fault we did not have a proper introduction <END>\n",
      "<BEG> no no it is my fault  we did not have a proper introduction <END>\n",
      "<BEG> cameron <END>\n",
      "<BEG> cameron <END>\n",
      "\n",
      "<BEG> gosh if only we could find kat a boyfriend <END>\n",
      "<BEG> gosh if only we could find kat a boyfriend <END>\n",
      "<BEG> let me see what i can do <END>\n",
      "<BEG> let me see what i can do <END>\n",
      "\n",
      "<BEG> <UNK> ma <UNK> this is my head <END>\n",
      "<BEG> c'esc ma tete this is my head <END>\n",
      "<BEG> right see you are ready for the <UNK> <END>\n",
      "<BEG> right  see  you are ready for the quiz <END>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show vocab_to_int mapping\n",
    "count = 0\n",
    "for a,b,c,d in zip(questions_int , short_questions , answers_int,short_answers):\n",
    "    count+=1\n",
    "    print(' '.join([questions_int_to_vocab[aa] for aa in a]))\n",
    "    print(b)\n",
    "    print(' '.join([questions_int_to_vocab[aa] for aa in c]))\n",
    "    print(d)\n",
    "    print()\n",
    "    if count>5:\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter sentences contain <UNK>\n",
    "sorted_questions_clean = []\n",
    "sorted_answers_clean = []\n",
    "\n",
    "for i in range(len(short_questions)):\n",
    "    if not(questions_vocab_to_int['<UNK>'] in questions_int[i] or answers_vocab_to_int['<UNK>'] in answers_int[i]): # remove '<UNK>' sentence\n",
    "        sorted_questions_clean.append(questions_int[i])\n",
    "        sorted_answers_clean.append(answers_int[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BEG> you are asking me out that is so cute that is your name again <END>\n",
      "<BEG> forget it <END>\n",
      "\n",
      "<BEG> no no it is my fault we did not have a proper introduction <END>\n",
      "<BEG> cameron <END>\n",
      "\n",
      "<BEG> gosh if only we could find kat a boyfriend <END>\n",
      "<BEG> let me see what i can do <END>\n",
      "\n",
      "<BEG> that is because it is such a nice one <END>\n",
      "<BEG> forget french <END>\n",
      "\n",
      "<BEG> there <END>\n",
      "<BEG> where <END>\n",
      "\n",
      "<BEG> you have my word as a gentleman <END>\n",
      "<BEG> you are sweet <END>\n",
      "\n",
      "<BEG> sure have <END>\n",
      "<BEG> i really really really wanna go but i cannot not unless my sister goes <END>\n",
      "\n",
      "<BEG> hi <END>\n",
      "<BEG> looks like things worked out tonight huh <END>\n",
      "\n",
      "<BEG> have fun tonight <END>\n",
      "<BEG> tons <END>\n",
      "\n",
      "<BEG> well no <END>\n",
      "<BEG> then that is all you had to say <END>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(' '.join([questions_int_to_vocab[sorted_questions_clean[i][j]]  for j in range(len(sorted_questions_clean[i]))]))\n",
    "    print(' '.join([answers_int_to_vocab[sorted_answers_clean[i][j]]  for j in range(len(sorted_answers_clean[i]))]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86564, 86564)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_questions_clean),len(sorted_answers_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 16\n"
     ]
    }
   ],
   "source": [
    "ques_max_len = 0\n",
    "ans_max_len = 0\n",
    "\n",
    "for i in range(len(sorted_questions_clean)): # caculate max length\n",
    "    ques_max_len = max(ques_max_len, len(sorted_questions_clean[i]))\n",
    "    ans_max_len = max(ans_max_len, len(sorted_answers_clean[i]))\n",
    "\n",
    "print(ques_max_len, ans_max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><b>Training (step1: generate batch、step2: Seq2Seq)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input\n",
      "Decoder input\n",
      "Decoder output\n",
      "\n",
      "you are asking me out that is so cute that is your name again <PAD> <PAD>\n",
      "<BEG> forget it <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "forget it <END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "no no it is my fault we did not have a proper introduction <PAD> <PAD> <PAD>\n",
      "<BEG> cameron <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "cameron <END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "gosh if only we could find kat a boyfriend <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<BEG> let me see what i can do <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "let me see what i can do <END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "that is because it is such a nice one <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<BEG> forget french <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "forget french <END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, en_corpus, ch_corpus, en_pad, ch_pad, en_max_len, ch_max_len, batch_size):\n",
    "#         assert len(en_corpus) == len(ch_corpus)\n",
    "        \n",
    "        batch_num = len(en_corpus)//batch_size\n",
    "        n = batch_num*batch_size\n",
    "        \n",
    "        self.xs = [np.zeros(n, dtype=np.int32) for _ in range(en_max_len)] # encoder inputs\n",
    "        self.ys = [np.zeros(n, dtype=np.int32) for _ in range(ch_max_len)] # decoder inputs\n",
    "        self.gs = [np.zeros(n, dtype=np.int32) for _ in range(ch_max_len)] # decoder outputs\n",
    "        self.ws = [np.zeros(n, dtype=np.float32) for _ in range(ch_max_len)] # decoder weight for loss caculation\n",
    "        \n",
    "        self.en_max_len = en_max_len\n",
    "        self.ch_max_len = ch_max_len\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        for b in range(batch_num):\n",
    "            for i in range(b*batch_size, (b+1)*batch_size):\n",
    "                for j in range(len(en_corpus[i])-2):\n",
    "                    self.xs[j][i] = en_corpus[i][j+1]\n",
    "                for j in range(j+1, en_max_len):\n",
    "                    self.xs[j][i] = en_pad\n",
    "                if ch_corpus:\n",
    "                    for j in range(len(ch_corpus[i])-1):\n",
    "                        self.ys[j][i] = ch_corpus[i][j]\n",
    "                        self.gs[j][i] = ch_corpus[i][j+1]\n",
    "                        self.ws[j][i] = 1.0\n",
    "                    for j in range(j+1, ch_max_len): # don't forget padding and let loss weight zero\n",
    "                        self.ys[j][i] = ch_pad\n",
    "                        self.gs[j][i] = ch_pad\n",
    "                        self.ws[j][i] = 0.0\n",
    "    \n",
    "    def get(self, batch_id):\n",
    "        x = [self.xs[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.en_max_len)]\n",
    "        y = [self.ys[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
    "        g = [self.gs[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
    "        w = [self.ws[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
    "        \n",
    "        return x, y, g, w\n",
    "\n",
    "batch = BatchGenerator(sorted_questions_clean, sorted_answers_clean, \n",
    "                       questions_vocab_to_int['<PAD>'], answers_vocab_to_int['<PAD>'], ques_max_len, ans_max_len, 4)\n",
    "\n",
    "print(\"Encoder input\")\n",
    "print(\"Decoder input\")\n",
    "print(\"Decoder output\")\n",
    "print()\n",
    "\n",
    "x, y, g, w = batch.get(0)\n",
    "for i in range(4):\n",
    "    print(' '.join([questions_int_to_vocab[x[j][i]] for j in range(ques_max_len)]))\n",
    "    print(' '.join([answers_int_to_vocab[y[j][i]] for j in range(ans_max_len)]))\n",
    "    print(' '.join([answers_int_to_vocab[g[j][i]] for j in range(ans_max_len)]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineTranslationSeq2Seq:\n",
    "    def __init__(self, en_max_len, ch_max_len, en_size, ch_size):\n",
    "        self.en_max_len = en_max_len\n",
    "        self.ch_max_len = ch_max_len\n",
    "        \n",
    "        with tf.variable_scope('seq2seq_intput/output'):\n",
    "            self.enc_inputs = [tf.placeholder(tf.int32, [None]) for i in range(en_max_len)] # time mojor feed\n",
    "            self.dec_inputs = [tf.placeholder(tf.int32, [None]) for i in range(ch_max_len)]\n",
    "            self.groundtruths = [tf.placeholder(tf.int32, [None]) for i in range(ch_max_len)]\n",
    "            self.weights = [tf.placeholder(tf.float32, [None]) for i in range(ch_max_len)]\n",
    "            \n",
    "        with tf.variable_scope('seq2seq_rnn'): # training by teacher forcing\n",
    "            self.out_cell = tf.contrib.rnn.LSTMCell(512)\n",
    "            self.outputs, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(self.enc_inputs, self.dec_inputs, \n",
    "                                                                                    self.out_cell, \n",
    "                                                                                    en_size, ch_size, 300)\n",
    "        with tf.variable_scope('seq2seq_rnn', reuse=True): # predict by feeding previous\n",
    "            self.pred_cell = tf.contrib.rnn.LSTMCell(512, reuse=True) # reuse cell for train and test\n",
    "            self.predictions, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(self.enc_inputs, self.dec_inputs, \n",
    "                                                                                        self.pred_cell, \n",
    "                                                                                        en_size, ch_size, 300, \n",
    "                                                                                        feed_previous=True)\n",
    "        \n",
    "        with tf.variable_scope('loss'):\n",
    "            # caculate weighted loss\n",
    "            self.loss = tf.reduce_mean(tf.contrib.legacy_seq2seq.sequence_loss_by_example(self.outputs, \n",
    "                                                                                          self.groundtruths, \n",
    "                                                                                          self.weights))\n",
    "            self.optimizer = tf.train.AdamOptimizer(0.002).minimize(self.loss)\n",
    "        \n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "        self.saver = tf.train.Saver()\n",
    "#         self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver.restore(self.sess, 'model/seq2seq/seq2seq_%d.ckpt'%(48))\n",
    "    def train(self, x, y, g, w):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i] # show how to feed a list\n",
    "        \n",
    "        for i in range(self.ch_max_len):\n",
    "            fd[self.dec_inputs[i]] = y[i]\n",
    "            fd[self.groundtruths[i]] = g[i]\n",
    "            fd[self.weights[i]] = w[i]\n",
    "        \n",
    "        loss, _ = self.sess.run([self.loss, self.optimizer], fd)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def output(self, x, y):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]\n",
    "        \n",
    "        for i in range(self.ch_max_len):\n",
    "            fd[self.dec_inputs[i]] = y[i]\n",
    "        \n",
    "        out = self.sess.run(self.outputs, fd)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def predict(self, x, ch_beg):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]\n",
    "        \n",
    "        for i in range(self.ch_max_len): # when feed previous, the fist token should be '<BEG>', and others are useless\n",
    "            if i==0:\n",
    "                fd[self.dec_inputs[i]] = np.ones(y[i].shape, dtype=np.int32)*ch_beg\n",
    "            else:\n",
    "                fd[self.dec_inputs[i]] = np.zeros(y[i].shape, dtype=np.int32)\n",
    "        \n",
    "        pd = self.sess.run(self.predictions, fd)\n",
    "        \n",
    "        return pd\n",
    "    \n",
    "    def save(self, e):\n",
    "        self.saver.save(self.sess, 'model/seq2seq/seq2seq_%d.ckpt'%(e+1))\n",
    "    \n",
    "    def restore(self, e):\n",
    "        self.saver.restore(self.sess, 'model/seq2seq/seq2seq_%d.ckpt'%(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/seq2seq/seq2seq_48.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model = MachineTranslationSeq2Seq(ques_max_len, ans_max_len, len(questions_vocab_to_int), len(answers_vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "batch_num = len(sorted_questions_clean)//BATCH_SIZE\n",
    "batch = BatchGenerator(sorted_questions_clean, sorted_answers_clean, \n",
    "                       questions_vocab_to_int['<PAD>'], answers_vocab_to_int['<PAD>'], ques_max_len, ans_max_len, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48 loss: 2.503017\n",
      "epoch 49 loss: 2.507307\n"
     ]
    }
   ],
   "source": [
    "rec_loss = []\n",
    "for e in range(48,EPOCHS):\n",
    "    train_loss = 0\n",
    "    \n",
    "    for b in range(batch_num):\n",
    "        x, y, g, w = batch.get(b)\n",
    "        batch_loss = model.train(x, y, g, w)\n",
    "        train_loss += batch_loss\n",
    "#         break\n",
    "    \n",
    "    train_loss /= batch_num\n",
    "    rec_loss.append(train_loss)\n",
    "    print(\"epoch %d loss: %f\" % (e, train_loss))\n",
    "    \n",
    "    model.save(e)\n",
    "    \n",
    "np.save('./model/seq2seq/rec_loss.npy', rec_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch 0 loss: 4.530785<br>\n",
    "epoch 1 loss: 4.076271<br>\n",
    "epoch 2 loss: 3.795061<br>\n",
    "epoch 4 loss: 3.437478<br>\n",
    "epoch 5 loss: 3.287869<br>\n",
    "epoch 6 loss: 3.165183<br>\n",
    "epoch 7 loss: 3.079307<br>\n",
    "epoch 8 loss: 3.003268<br>\n",
    "epoch 9 loss: 2.934210<br>\n",
    "epoch 10 loss: 2.879893<br>\n",
    "epoch 11 loss: 2.849603<br>\n",
    "epoch 12 loss: 2.800245<br>\n",
    "epoch 13 loss: 2.775303<br>\n",
    "epoch 14 loss: 2.760754<br>\n",
    "epoch 15 loss: 2.726841<br>\n",
    "epoch 16 loss: 2.719413<br>\n",
    "epoch 17 loss: 2.753609<br>\n",
    "epoch 18 loss: 2.757131<br>\n",
    "epoch 19 loss: 2.678192<br>\n",
    "epoch 20 loss: 2.669429<br>\n",
    "epoch 21 loss: 2.669876<br>\n",
    "epoch 22 loss: 2.663899<br>\n",
    "epoch 23 loss: 2.657978<br>\n",
    "epoch 24 loss: 2.640877<br>\n",
    "epoch 25 loss: 2.613516<br>\n",
    "epoch 26 loss: 2.598924<br>\n",
    "epoch 27 loss: 2.643231<br>\n",
    "epoch 28 loss: 2.588912<br>\n",
    "epoch 29 loss: 2.576181<br>\n",
    "epoch 30 loss: 2.566805<br>\n",
    "epoch 31 loss: 2.555187<br>\n",
    "epoch 32 loss: 2.559543<br>\n",
    "epoch 33 loss: 2.559104<br>\n",
    "epoch 34 loss: 2.522690<br>\n",
    "epoch 35 loss: 2.532498<br>\n",
    "epoch 36 loss: 2.547435<br>\n",
    "epoch 37 loss: 2.539630<br>\n",
    "epoch 38 loss: 2.564870<br>\n",
    "epoch 39 loss: 2.534656<br>\n",
    "epoch 40 loss: 2.546252<br>\n",
    "epoch 41 loss: 2.530325<br>\n",
    "epoch 42 loss: 2.548503<br>\n",
    "epoch 43 loss: 2.547191<br>\n",
    "epoch 44 loss: 2.533915<br>\n",
    "epoch 45 loss: 2.512516<br>\n",
    "epoch 46 loss: 2.540780<br>\n",
    "epoch 47 loss: 2.494065<br>\n",
    "epoch 48 loss: 2.503017<br>\n",
    "epoch 49 loss: 2.507307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/seq2seq/seq2seq_49.ckpt\n"
     ]
    }
   ],
   "source": [
    "# restore model weights\n",
    "model.restore(49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><b>test for five sentences:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "testInput = [\"Hello.\",\n",
    "\"How are you?\",\n",
    "\"Where are you going?\",\n",
    "\"You look great.\",\n",
    "\"Good night.\"]\n",
    "tests = []\n",
    "for test in testInput:\n",
    "    tmp = []\n",
    "    for id  in ('<BEG> '+clean_text(test)+' <END>').split():\n",
    "        tmp.append(questions_vocab_to_int[id])\n",
    "    tests.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = BatchGenerator(tests, [], \n",
    "                       questions_vocab_to_int['<PAD>'], answers_vocab_to_int['<PAD>'], ques_max_len, ans_max_len, len(testInput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs,y,z,w = batch.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "how are you <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "where are you going <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "you look great <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "good night <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  show test inputs' batch result\n",
    "for i in range(len(testInput)):\n",
    "    print(' '.join([questions_int_to_vocab[xs[j][i]] for j in range(ques_max_len)]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "records = []\n",
    "\n",
    "pd = model.predict(xs,answers_vocab_to_int['<PAD>'])\n",
    "for j in range(len(pd[0])):\n",
    "    ch_pd = [answers_int_to_vocab[ np.argmax(pd[i][j,:]) ] for i in range(ans_max_len)]\n",
    "    if '<END>' in ch_pd:\n",
    "        ch_pd = ch_pd[:ch_pd.index('<END>')]\n",
    "    records.append(ch_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello.\n",
      "['surprised']\n",
      "\n",
      "How are you?\n",
      "['fine']\n",
      "\n",
      "Where are you going?\n",
      "['home']\n",
      "\n",
      "You look great.\n",
      "['thanks']\n",
      "\n",
      "Good night.\n",
      "['good', 'boy']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for id,i in enumerate(records):\n",
    "    print(testInput[id])\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><b>explain result:</b></font>\n",
    "\n",
    "Hello.\n",
    "['surprised']\n",
    "\n",
    "一個壽星到了和朋友約的建築物，門是微打開的，於是他有點害怕的走進去走進黑暗的房間，說：Hello\n",
    "\n",
    "朋友們立馬跳出來拿著插滿蠟燭的蛋糕，說：surprised\n",
    "\n",
    "How are you?\n",
    "['fine']\n",
    "\n",
    "Where are you going?\n",
    "['home']\n",
    "\n",
    "You look great.\n",
    "['thanks']\n",
    "\n",
    "Good night.\n",
    "['good', 'boy']\n",
    "\n",
    "晚上媽媽坐在小男孩身邊哄他睡覺，小男孩快睡著時，輕聲跟媽媽說：good night\n",
    "\n",
    "媽媽親吻他的額頭說：good boy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
